{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATHS1004 Mathematics for Data Science I\n",
    "## Computer Lab 6\n",
    "\n",
    "Welcome to the final computer lab! In this lab we'll pull together a few of the pieces we've learned throughout this series of labs, and also delve a little deeper into naive Bayes classifiers.\n",
    "\n",
    "But first:\n",
    "\n",
    "## A warmup activity\n",
    "\n",
    "In Week 10 we calculated the expected value of winnings for X Lotto, based on buying a single ticket. The facts were:\n",
    "- Each draw consists of 6 random numbers drawn from a barrel containing the numbers 1-45;\n",
    "- A single ticket costs $4.20;\n",
    "- A single ticket gives you 6 entries (i.e., 6 sets of 6 numbers);\n",
    "- You win the jackpot if you get all 6 numbers drawn correct, and lose the cost of your ticket otherwise.\n",
    "\n",
    "Based on this we calculated that your exepected winnings are negative, even when the jackpot is as high as $4 million.\n",
    "\n",
    "**Question**: What do your expected winnings look like if you buy multiple tickets? Is there a point at which you might expect to make a profit?\n",
    "\n",
    "Answer this question by creating a function `expected_winnings(jackpot,ticket_price,num_plays)`, which uses the definition of expectation for a discrete random variable to calculate the expected winnings given a `jackpot` value, cost of a single ticket `ticket_price`, and number of tickets purchased `num_plays`.\n",
    "- Use your function to plot expected winnings as a function of number of tickets purchased, for the jackpot and ticket prices given above. What happens to your expected winnings as you buy more tickets?\n",
    "- At what value of the `jackpot` does playing X Lotto start to become profitable?\n",
    "- How cheap would tickets need to become for you to want to purchase a ticket in a $4M draw?\n",
    "\n",
    "You'll need to use the [scipy function for the binomial coefficient](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.special.binom.html), so I'll load that for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "\n",
    "def expected_winnings(jackpot,ticket_price,num_plays):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classification in practice\n",
    "\n",
    "Let's see how Naive Bayes can be used to predict the rating of film reviews, using a famous dataset. The [IMDB reviews dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) contains 50k reviews from IMDB.com. Download the csv file that came with this lab, put it in an appropriate directory (remember having this fun in Lab 1?), and then load it using `pandas` (I've added a few options so the output looks more readable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 280)\n",
    "\n",
    "df = pd.read_csv('PATH_TO_imdb_master.csv', encoding=\"ISO-8859-1\",index_col=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains positive, negative, and neutral reviews. We'll just consider the postive and negative reviews, and a small sample for now. Create a new dataframe `dfpn` containing just the reviews having `label` == `pos` or `label` == `neg`. Then execute the cell underneath to take a sample of just 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfpn.sample(100,random_state=19)\n",
    "dfs.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train a model! The dataset came with a split of the data into training + testing, so let's use that. Create variables `docs_train` and `docs_test` containing the reviews where the `type` is 'train' and 'test', respectively. Do the same for the labels, to create `labels_train` and `labels_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a lot of text data, but how should we convert this into counts, like we did for spam filtering? More importantly, which words or phrases should we use? The beauty of a NB classifier is that it's robust to using *all* the data, so we'll just use a handy function to count instances of *all* words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokeniser = CountVectorizer()\n",
    "\n",
    "counts_train = tokeniser.fit_transform(docs_train.str.lower())\n",
    "counts_test = tokeniser.transform(docs_test.str.lower())\n",
    "\n",
    "print(counts_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last lines convert to a large sparse matrix of counts. (How many features or predictors do we have here?) And now we can use those counts `counts_train` along with the labels `labels_train` to \"fit\" our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(counts_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy as that! Now we can make predictions on the unseen `test` data. Let's also take a look at the actual probabilities predicted by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(counts_test)\n",
    "p_pred = clf.predict_proba(counts_test)\n",
    "\n",
    "\n",
    "print('predict / actual / probabilities')\n",
    "print()\n",
    "for a,b,c in zip(y_pred,y_test,p_pred):\n",
    "    print(a,'\\t',b,'\\t',c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to look at those probabilities. Notice that the model makes a lot of mistakes, and sometimes is highly overconfident about those mistakes (for example, the very first line), but other times is much less confident.\n",
    "\n",
    "A nice property of NB compared with other machine learning algorithms is that you can look at the relative probabilities of each word to predict the two classes. These are contained in `clf.feature_log_prob_` and we can use them as below to see the top words predicting the positive versus negative classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_ratio_sorted = (clf.feature_log_prob_[0, :]/clf.feature_log_prob_[1, :]).argsort()\n",
    "\n",
    "\n",
    "n_features = 20\n",
    "print('Top words for \"negative\" class:')\n",
    "print(np.take(tokeniser.get_feature_names(), log_prob_ratio_sorted[:n_features]))\n",
    "print()\n",
    "print('Top words for \"positive\" class:')\n",
    "print(np.take(tokeniser.get_feature_names(), log_prob_ratio_sorted[-n_features:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not that great, and there's a lot of randomness here, but already we see a few words that \"pass the stupidity test\": \"awful\" for the negative class, and \"liked\" and \"wonderful\" for the positive.\n",
    "\n",
    "Finally, how to summarise all of this information? It's useful to look at a [report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) of measures like accuracy (the proportion of correct predictions), as well as things like *precision*, *recall*, etc (which you'll hear more about as you progress further into data science). The [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) is particularly useful, telling us the number of misclassifications we made in the off-diagonal elements. I'll import the functions, and then you write the commands to use them on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall: our model is not bad, particularly given that we only used 100 out of 50K reviews! Now:\n",
    "- Go back and change the sample of 100 to, say, 1000 reviews. How does the model improve? Make sure you look not just at the accuracy etc, but the informative features as well.\n",
    "- What about 10,000 reviews? See how things improve then.\n",
    "- You might like to make a plot of how the accuracy improves as the amount of data increases. How about the other measures like precision, recall, and F1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for continuous data\n",
    "\n",
    "Naive Bayes has plenty of application to text and other discrete forms of data, but what if my data is continuous? How do I calculate any of the probabilities $P()$ in \n",
    "\n",
    "$$\n",
    "P(c|x_1,x_2,\\ldots,x_n) = \\frac{P(x_1 | c) P(x_2|c) \\ldots P(x_n | c) P(c)}{P(x_1,x_2,\\ldots,x_n)}\n",
    "$$\n",
    "\n",
    "if the $x_i$'s are continuous?\n",
    "\n",
    "The answer is to use one of the models for *continuous random variables* (our lecture topic in Week 11), of which normal (or *Gaussian*) random variables are by far the most common.\n",
    "\n",
    "We'll demonstrate using the very famous [iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) first introduced by the very famous Ronald Fisher (who happened to live at the University of Adelaide at the end of his life and is buried in St Peter's Cathedral just over the river from here!). Load it up from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Shuffle the rows to make training easier later on\n",
    "data = np.zeros((len(y),5))\n",
    "data[:,:-1] = X\n",
    "data[:,-1] = y\n",
    "np.random.shuffle(data)\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is to predict which of the 3 types of iris (Setosa, Versicolour, and Virginica) the 150 samples are, given the 4 predictors (Sepal Length, Sepal Width, Petal Length and Petal Width).\n",
    "\n",
    "First, let's see if there is any structure in this dataset! Have an explore, by making a few different scatter plots of the different predictors. Colour your points by the type of iris to see if there is any structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that there is some clustering, but how to spot this automatically?\n",
    "\n",
    "PCA, of course! Using the skills you learnt in Lab 4, do a Principal Component Analysis of the iris dataset, and plot PC1 versus PC2. You should be able to see clusters!\n",
    "\n",
    "(You can follow the procedure from Lab 4, or you might want to look at `sklearn`'s PCA function, which I used in lectures -- the notebook is included with this lab.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely clusters there! We should be able to train a NB model to classify irises.\n",
    "\n",
    "Using the process from above, but this time importing and using `GaussianNB` instead of `MultinomialNB`, and using the first 100 rows of the data in `X` to train the model, create a NB classification model to predict iris type. Use `sklearn` to report on the accuracy etc of the model, plus look at the confusion matrix to see which classes were misclassified. \n",
    "\n",
    "Overall your model should be performing pretty well! From your exploratory data analysis earlier, can you see why those particular types of iris were more difficult to distinguish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a principled, interpretable, and often-overlooked data science tool. Challenge: You might like to see how it performs on some other examples we've encountered during this course, e.g.,:\n",
    "- the breast cancer dataset (also using continuous data);\n",
    "- the Titanic dataset (which contains both continuous and discrete data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
