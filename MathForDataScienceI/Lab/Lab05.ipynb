{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATHS1004 Mathematics for Data Science I\n",
    "## Computer Lab 5\n",
    "\n",
    "This lab is partly about text generation using Markov chains, but also (really) an illustration of *data wrangling* -- one of the essential aspects of practical data science. Actual data scientists say that wrangling data into a useable form is [80% of the job](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html)! \n",
    "\n",
    "Along the way we'll learn about:\n",
    "- reading data into Python from a URL;\n",
    "- dealing with strings;\n",
    "- dictionaries! (Possibly **the best** aspect of using Python);\n",
    "- problem-solving with messy data.\n",
    "\n",
    "Here we go. \n",
    "\n",
    "### Data wrangling\n",
    "\n",
    "In this section we need to read a piece of text in, clean it, then create a list of words from it. We'll need the following libraries, so load them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request, string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`urrllib.request` allows you to access data directly from a URL. The following block loads a text file from the URL given, and then reads the first line. Run it to find out the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL = 'http://www.site.uottawa.ca/~lucia/courses/2131-02/A2/trythemsource.txt'\n",
    "\n",
    "data = urllib.request.urlopen(URL)\n",
    "data.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun!!! Part of the reason this book is famous is because it [uses only 50 different words](https://en.wikipedia.org/wiki/Green_Eggs_and_Ham), making it perfect for exploring text generation.\n",
    "\n",
    "The first thing you notice here is that we have a weird `b` before the string prints. A little bit of Googling will tell you that this means the string is encoded in a format called \"UTF-8\". This will cause us pain later on! So the first step will be to decode it to a regular ASCII string. Copy the last two commands above into the cell below, then put a `.decode('utf-8')` at the end of the `data.readline()` command, then execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = urllib.request.urlopen(URL)\n",
    "data.readline().decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `b` should be gone -- great! We now have an ASCII string.\n",
    "\n",
    "Now, let's look at the whole document, line by line. Loop over the entire object `data`, and print each line. I'll start you off, then you fill in the rest of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the list of words we need for this project the best thing to do is to first make this whole document into one long string. So we need to append each line into one string. Fortunately, you can do this in Python easily by adding strings. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Do you like green eggs and Ham?\"\n",
    "line = \"I do not like them, Sam-I-Am.\"\n",
    "\n",
    "text = text + line\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Question: How could you put in a space between the first and second sentences without changing them?)\n",
    "\n",
    "Try using this idea to build your own variable `text` containing the entire document. We'll need to reload from the `URL`, so again I'll start you off. You fill in the rest of the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = urllib.request.urlopen(URL)\n",
    "data.readline().decode('utf-8')\n",
    "\n",
    "text = ''\n",
    "for line in data:\n",
    "\n",
    "    \n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got an error: did you decode each line to an ASCII string? Make sure you use the `decode` function.\n",
    "\n",
    "Once you've got the long `text`, you'll notice another issue -- we have `\\r`'s and `\\n`'s all over the place! These are newline characters, which will put gaps in if you `print(text)` (try it). But they're a problem! You'll need to strip them out. Fortunately there's another helpful function: appending `.rstrip()` will strip out special characters from the right of a string, leaving us with only the text. Put this in to your loop above to end up with just text.\n",
    "\n",
    "\n",
    "Once `text` contains only text, we move on to the next issue: to do text generation we need to have only words, no punctuation! So we should replace every piece of punctuation with a space. You can Google how to do this, or I can do it for you and find the code snippet I found..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "text = text.translate(translator)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking pretty close. But can you see an issue? Look on the 6th line from the bottom, do you see some words in lower case? Whoever made this text file has added something in there, a description of a picture from the book. How annoying for us! Those words aren't part of the original story, and so we should remove them. Look up the [replace](https://www.geeksforgeeks.org/python-string-replace/) command, and use it to remove that piece of text. Then, let's convert everything to lower-case, by appending `.lower()` to the end of `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now. How many unique words are there in `text`? The code below splits `text` (by spaces) into a list of words. Then I create a set from that list (i.e., all the unique elements), and take the length. This gives the vocabulary size, or the number of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = text.split()\n",
    "len(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get 52 different words. What??? Where did those extra 2 words come from? Something is messy with the dataset itself, and we have to do some legit detective work to track the issue down. Here's how I figured it out. `Counter` creates a dictionary of how many times each item appears in a list. And `.most_common()` sorts that list from most to least common. \n",
    "\n",
    "Run the cell below, then look towards the bottom. I won't spoil your fun -- go figure out what the origin of those two extra words is. (Hint: You might need to visit the original URL and do some searching/reading...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've figured it out, go ahead and `.replace` those two words with what they ought to be! Then check that the vocabulary size is in fact 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this lab, we can now consider this dataset *cleaned*, and we can proceed to doing some text generation using the list `words`. Think about the steps involved with the cleaning, the puzzles to solve, and the care we had to take. This is the essence of doing data science in practice.\n",
    "\n",
    "Let's now move on to the original aim of the lab!\n",
    "\n",
    "### Text generation using Markov chains\n",
    "\n",
    "Text generation is important in modern tech -- it forms the basis for predictive text on your smartphone, as well as [topic modelling](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf), image captioning, and text summarisation. Recently a computational model called GPT-2 was deemed \"[too dangerous to release](https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction)\" by its own creators!\n",
    "\n",
    "We will take no such precautions here.\n",
    "\n",
    "One foundation of text generation is Markov chains, a stochastic model where the probability of being in a particular state at time $k+1$ depends on where you were at time $k$:\n",
    "$$\n",
    "{\\bf x}_{k+1} = P{\\bf x}_k\n",
    "$$\n",
    "In a Markov chain, the vectors ${\\bf x}_k$ are called *state* vectors and have the property that each\n",
    "entry lies between 0 and 1 and their sum is one.  Vectors with this property\n",
    "are called *probability* vectors.  The matrix $P$ is a square matrix\n",
    "all of whose columns are probability vectors; such a matrix $P$ is called a\n",
    "*stochastic* matrix.\n",
    "\n",
    "We've already seen an example of a Markov chain: the *PageRank* example we did in lectures (from [here](https://www.rose-hulman.edu/~bryan/googleFinalVersionFixed.pdf)) was a Markov chain. Here's another one. \n",
    "\n",
    "Let ${\\bf x}_0=(0.2,0.8)$ represent the\n",
    "proportions of a population in the city and suburbs at time $k=0$.  The\n",
    "transition from one year to the next can be given as a $2\\times 2$ matrix:\n",
    "\n",
    "$$\n",
    "    P = \\begin{array}{c@{}l}\n",
    "        \\text{From:} & \\text{To:} \\\\\n",
    "        \\text{City Suburbs} & \\\\\n",
    "        \\begin{bmatrix} \n",
    "         \\ 0.97\\ &\\ 0.05\\ \\\\\n",
    "         0.03 & 0.95\\end{bmatrix} \n",
    "         & \\begin{array}{l} \\text{City}\\\\ \n",
    "         \\text{Suburbs}\\end{array} \n",
    "         \\end{array}\n",
    "$$\n",
    "For $k=0,1,2,3,\\dots$ we have ${\\bf x}_{k+1}=P{\\bf x}_k$.\n",
    "\n",
    "Try using `numpy` and a `for` loop to calculate the proportions of population in city versus suburbs for a few years. What do the proportions converge to over time? (If you're interested in the theory behind this, PageRank, and Markov chains more generally, come back for the course MATHS 2103/7103 *Probability & Statistics*!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this relate to text generation? Well, we can use our list `words` above to build a transition matrix $P$, make sure that the columns sum to 1 (think about how you might do this), and then simulate new text.\n",
    "\n",
    "Of course, it's not very efficient to build a $50\\times50$ transition matrix, and it doesn't scale well to larger vocabularies. A much more elegant approach is to build a *dictionary* of all the words appearing after each word in our text, and then sample from the lists in this dictionary. I adapted the following code to build this dictionary from [this nice tutorial](https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_markov_chain_model(words):\n",
    "    word_dict = {}\n",
    "    for i in range(len(words)-1):\n",
    "        word_1 = words[i]\n",
    "        word_2 = words[i+1]\n",
    "        if word_1 in word_dict.keys():\n",
    "            word_dict[word_1].append(word_2)\n",
    "        else:\n",
    "            word_dict[word_1] = [word_2]\n",
    "            \n",
    "    return(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function above to build a dictionary from the `words` list you built above. Take a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate text, we start with a word then sample from the list of words following it in our dictionary to create new text. Here's a function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_text(n_words,first_word,word_dict):\n",
    "    chain = [first_word]\n",
    "    for i in range(n_words):\n",
    "        chain.append(np.random.choice(word_dict[chain[-1]]))\n",
    "\n",
    "    simulated_text = ' '.join(chain)\n",
    "    return(simulated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a starting word an use this function to simulate, say, 50 words of original \"Green Eggs and Ham\" text. How does it read? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions for you to explore:\n",
    "- What happens if you use different starting words? For example, try starting with \"if\".\n",
    "- What happens if you use a word that wasn't in the original text?\n",
    "- How might you incorporate sentence structure, rather than just arbitrary lengths of text?\n",
    "- How might you make the generated text more realistic? (You might like to look up what a \"higher-order\" Markov chain is.) Can you think of any computational issues that might arise here?\n",
    "- What about generating text in the style of different authors? You might want to look into \"training\" a Markov chain model on works from [Project Gutenberg](https://www.gutenberg.org), which can be accessed [directly](https://pypi.org/project/Gutenberg/) from Python.\n",
    "\n",
    "There's also a nice package called [markovify](https://github.com/jsvine/markovify) which makes building the Markov chains earier. Check it out!\n",
    "\n",
    "Finally, notice how Markov chains blend two different topics together: linear algebra and probability. This is another characteristic of practical data science -- it uses a broad range of mathematical skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
